{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The Model That Have Results Public 0.72 - Private 0.55\n"
      ],
      "metadata": {
        "id": "VVYJpMJM1k10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyw9OzPE1dJS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.utils import shuffle\n",
        "import nltk\n",
        "\n",
        "# Load NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('bugs-train.csv')\n",
        "test_data = pd.read_csv('bugs-test.csv')\n",
        "# Initialize Sentence-BERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Get Sentence-BERT embeddings for train and test data\n",
        "X_train_bert = model.encode(train_data['summary'].tolist(), show_progress_bar=True, batch_size=64)\n",
        "X_test_bert = model.encode(test_data['summary'].tolist(), show_progress_bar=True, batch_size=64)\n",
        "\n",
        "# Map severity to numerical values\n",
        "severity_mapping = {'trivial': 0, 'enhancement': 1, 'minor': 2, 'normal': 3, 'major': 4, 'blocker': 5, 'critical': 6}\n",
        "train_data['severity'] = train_data['severity'].map(severity_mapping).dropna()\n",
        "y_train = train_data['severity'].values\n",
        "\n",
        "# Shuffle the data\n",
        "X_train_combined, y_train = shuffle(X_train_bert, y_train, random_state=42)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate sample weights\n",
        "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "# Define the parameter grid for Logistic Regression\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['lbfgs', 'liblinear'],\n",
        "    'max_iter': [100, 200, 300, 500]\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logreg = LogisticRegression(random_state=42)\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid,\n",
        "                           scoring='accuracy', cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model with the best parameters\n",
        "grid_search.fit(X_train_combined, y_train, sample_weight=sample_weights)\n",
        "\n",
        "# Get the best estimator\n",
        "best_logreg = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = best_logreg.predict(X_test_bert)\n",
        "\n",
        "# Convert numerical predictions back to severity strings\n",
        "severity_mapping_inverse = {v: k for k, v in severity_mapping.items()}\n",
        "test_data['severity'] = [severity_mapping_inverse[pred] for pred in test_predictions]\n",
        "\n",
        "# Prepare the result file in the format of the sample solution\n",
        "result = test_data[['bug_id', 'severity']].copy()\n",
        "result_path = 'bugs-pred.csv'\n",
        "result.to_csv(result_path, index=False)\n",
        "\n",
        "# Print out the best parameters and the best score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: \", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate sample weights\n",
        "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "# Initialize the Logistic Regression model with specified parameters\n",
        "logreg = LogisticRegression(random_state=42, C=1, max_iter=500, solver='liblinear')\n",
        "\n",
        "# Fit the model\n",
        "logreg.fit(X_train_combined, y_train, sample_weight=sample_weights)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = logreg.predict(X_test_bert)\n",
        "\n",
        "# Convert numerical predictions back to severity strings\n",
        "severity_mapping_inverse = {v: k for k, v in severity_mapping.items()}\n",
        "test_data['severity'] = [severity_mapping_inverse[pred] for pred in test_predictions]\n",
        "\n",
        "# Prepare the result file in the format of the sample solution\n",
        "result = test_data[['bug_id', 'severity']].copy()\n",
        "result_path = 'bugs-pred.csv'\n",
        "result.to_csv(result_path, index=False)\n",
        "\n",
        "# Since GridSearchCV is removed, we do not have best parameters or score\n",
        "print(\"Model trained with C=1, max_iter=500, solver='liblinear'\")\n"
      ],
      "metadata": {
        "id": "qbhYOInz10EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[\"severity\"].value_counts()"
      ],
      "metadata": {
        "id": "ptoV1U3a10sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Model That Have Results Public 0.70 - Private 0.60\n"
      ],
      "metadata": {
        "id": "6vKoEmwN12S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.utils import shuffle\n",
        "import nltk\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('bugs-train.csv')\n",
        "test_data = pd.read_csv('bugs-test.csv')\n",
        "\n",
        "# Text preprocessing function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "train_data['summary'] = train_data['summary'].apply(preprocess_text)\n",
        "test_data['summary'] = test_data['summary'].apply(preprocess_text)\n",
        "\n",
        "# Initialize Sentence-BERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Get Sentence-BERT embeddings for train and test data\n",
        "X_train_bert = model.encode(train_data['summary'].tolist(), show_progress_bar=True, batch_size=64)\n",
        "X_test_bert = model.encode(test_data['summary'].tolist(), show_progress_bar=True, batch_size=64)\n",
        "\n",
        "# Map severity to numerical values\n",
        "severity_mapping = {'trivial': 0, 'enhancement': 1, 'minor': 2, 'normal': 3, 'major': 4, 'blocker': 5, 'critical': 6}\n",
        "train_data['severity'] = train_data['severity'].map(severity_mapping).dropna()\n",
        "y_train = train_data['severity'].values\n",
        "\n",
        "# Shuffle the data\n",
        "X_train_combined, y_train = shuffle(X_train_bert, y_train, random_state=42)\n",
        "\n",
        "# Model training with XGBoost\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
        "param_dist_xgb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "random_search_xgb = RandomizedSearchCV(xgb_model, param_distributions=param_dist_xgb, n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search_xgb.fit(X_train_combined, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = random_search_xgb.predict(X_test_bert)\n",
        "\n",
        "# Convert numerical predictions back to severity strings\n",
        "severity_mapping_inverse = {v: k for k, v in severity_mapping.items()}\n",
        "test_data['severity'] = [severity_mapping_inverse[pred] for pred in test_predictions]\n",
        "\n",
        "# Evaluate model performance\n",
        "print(classification_report(y_test, test_predictions, target_names=severity_mapping_inverse.values()))\n",
        "\n",
        "# Prepare the result file in the format of the sample solution\n",
        "result = test_data[['bug id', 'severity']].copy()\n",
        "result_path = 'bugs-pred.csv'\n",
        "result.to_csv(result_path, index=False)\n"
      ],
      "metadata": {
        "id": "PI_T19r017ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Model That Have Results Public 0.67 - Private 0.67\n"
      ],
      "metadata": {
        "id": "Q4cMaaJd19wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "train_data = pd.read_csv('bugs-train (1).csv')\n",
        "test_data = pd.read_csv('bugs-test (1).csv')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "train_data['summary'] = train_data['summary'].apply(preprocess_text)\n",
        "test_data['summary'] = test_data['summary'].apply(preprocess_text)\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "X_train_bert = model.encode(train_data['summary'].tolist(), show_progress_bar=True, batch_size=64)\n",
        "X_test_bert = model.encode(test_data['summary'].tolist(), show_progress_bar=True, batch_size=64)\n",
        "\n",
        "severity_mapping = {'trivial': 0, 'enhancement': 1, 'minor': 2, 'normal': 3, 'major': 4, 'blocker': 5, 'critical': 6}\n",
        "train_data['severity'] = train_data['severity'].map(severity_mapping).dropna()\n",
        "y_train = train_data['severity'].values\n",
        "\n",
        "X_train_combined, y_train = shuffle(X_train_bert, y_train, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_combined)\n",
        "pca = PCA(n_components=100, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_scaled = scaler.transform(X_test_bert)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "X_train_pca, X_val_pca, y_train, y_val = train_test_split(X_train_pca, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "random_forest = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "xgboost = XGBClassifier(objective='multi:softmax', num_class=7, eval_metric='mlogloss', use_label_encoder=False, random_state=42)\n",
        "svm = SVC(kernel='rbf', C=1, random_state=42)\n",
        "\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('RandomForest', random_forest),\n",
        "    ('XGBoost', xgboost),\n",
        "    ('SVM', svm)\n",
        "], voting='hard', n_jobs=-1)\n",
        "\n",
        "print(\"Ensemble modelini eğitme...\")\n",
        "ensemble_model.fit(X_train_pca, y_train)\n",
        "y_pred_val = ensemble_model.predict(X_val_pca)\n",
        "\n",
        "print(\"Ensemble modeli performansı:\")\n",
        "print(classification_report(y_val, y_pred_val, target_names=severity_mapping.keys()))\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred_val, average='macro')\n",
        "results = {\n",
        "    'Ensemble': {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "}\n",
        "\n",
        "y_pred_test = ensemble_model.predict(X_test_pca)\n",
        "\n",
        "test_data = pd.read_csv('bugs-test (1).csv')\n",
        "test_data['Ensemble_severity'] = y_pred_test\n",
        "reverse_severity_mapping = {v: k for k, v in severity_mapping.items()}\n",
        "test_data['Ensemble_severity'] = test_data['Ensemble_severity'].map(reverse_severity_mapping)\n",
        "\n",
        "submission = test_data[['bug_id', 'Ensemble_severity']]\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "print(submission.head())"
      ],
      "metadata": {
        "id": "LnCN3hnB2BX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}